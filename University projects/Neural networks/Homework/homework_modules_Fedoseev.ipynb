{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":9891,"status":"ok","timestamp":1743336204852,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"3MeKai5Xj6eX"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import Tensor\n","from typing import Optional\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","import numpy\n","import unittest"]},{"cell_type":"markdown","metadata":{"id":"wwlFrG-Tj6eY"},"source":["**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments. \\\n","**Module** - это абстрактный класс, который определяет фундаментальные методы, необходимые для обучения нейронной сети. Вам не нужно ничего здесь менять, просто прочитайте комментарии."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":83,"status":"ok","timestamp":1743336204854,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"W8BLmtZ3j6eZ"},"outputs":[],"source":["class Module(object):\n","    \"\"\"\n","    Basically, you can think of a module as of a something (black box)\n","    which can process `input` data and produce `ouput` data.\n","    This is like applying a function which is called `forward`:\n","\n","        output = module.forward(input)\n","\n","    The module should be able to perform a backward pass: to differentiate the `forward` function.\n","    More, it should be able to differentiate it if is a part of chain (chain rule).\n","    The latter implies there is a gradient from previous step of a chain rule.\n","\n","        gradInput = module.backward(input, gradOutput)\n","    \"\"\"\n","    def __init__ (self):\n","        self.output = None\n","        self.gradInput = None\n","        self.training = True\n","\n","        # self.output — хранит результат прямого прохода (выходное значение).\n","        # self.gradInput — хранит градиент по входу (нужен для обратного прохода).\n","        # self.training — флаг, который показывает, работает ли модель в режиме обучения (True) или в режиме оценки (False).\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Takes an input object, and computes the corresponding output of the module.\n","        \"\"\"\n","        return self.updateOutput(input) # данный метод есть чуть ниже\n","\n","        # Вызывает updateOutput(input), которая должна быть переопределена в дочерних классах.\n","        # Задача: вычислить выходное значение (output) на основе входа.\n","\n","    def backward(self, input, gradOutput):\n","        \"\"\"\n","        Performs a backpropagation step through the module, with respect to the given input.\n","\n","        This includes\n","         - computing a gradient w.r.t. `input` (is needed for further backprop),\n","         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n","        \"\"\"\n","        self.updateGradInput(input, gradOutput) # метод, лежит ниже\n","        self.accGradParameters(input, gradOutput) # метод, лежи ниже\n","        return self.gradInput\n","\n","        # Вызывает два метода:\n","        #   updateGradInput(input, gradOutput) — вычисляет градиент по входу.\n","        #   accGradParameters(input, gradOutput) — вычисляет градиент по параметрам (если есть параметры).\n","        # Возвращает self.gradInput (градиент по входу).\n","\n","\n","\n","    def updateOutput(self, input):\n","        \"\"\"\n","        Computes the output using the current parameter set of the class and input.\n","        This function returns the result which is stored in the `output` field.\n","\n","        Make sure to both store the data in `output` field and return it.\n","\n","        Это напоминание о том, что метод updateGradInput должен:\n","        Сохранить градиент во внутренней переменной self.gradInput.\n","        Вернуть этот градиент (так как backward() ожидает его).\n","        \"\"\"\n","\n","\n","        # The easiest case:\n","\n","        # self.output = input\n","        # return self.output\n","\n","        pass\n","\n","        # Должна быть переопределена в наследниках!\n","        # Вычисляет self.output на основе input и возвращает его.\n","\n","    def updateGradInput(self, input, gradOutput):\n","        \"\"\"\n","        Computing the gradient of the module with respect to its own input.\n","        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n","\n","        The shape of `gradInput` is always the same as the shape of `input`.\n","\n","        Make sure to both store the gradients in `gradInput` field and return it.\n","        \"\"\"\n","\n","        # The easiest case:\n","\n","        # self.gradInput = gradOutput\n","        # return self.gradInput\n","\n","        pass\n","\n","        # Должна быть переопределена в наследниках!\n","        # Вычисляет градиент self.gradInput по входу.\n","        # Градиент по входу имеет ту же форму, что и сам вход.\n","\n","    def accGradParameters(self, input, gradOutput):\n","        \"\"\"\n","        Computing the gradient of the module with respect to its own parameters.\n","        No need to override if module has no parameters (e.g. ReLU).\n","        \"\"\"\n","        pass\n","\n","    # Вычисляет градиент по параметрам модуля.\n","    # Если у модуля нет параметров (например, ReLU), переопределять не нужно.\n","\n","    def zeroGradParameters(self):\n","        \"\"\"\n","        Zeroes `gradParams` variable if the module has params.\n","        \"\"\"\n","        pass\n","\n","    # Обнуляет градиенты параметров (нужно перед каждым шагом оптимизации).\n","\n","    def getParameters(self):\n","        \"\"\"\n","        Returns a list with its parameters.\n","        If the module does not have parameters return empty list.\n","        \"\"\"\n","        return []\n","\n","    # Возвращает список параметров (например, веса W и B).\n","    # Если у слоя нет параметров (ReLU), возвращает [].\n","\n","    def getGradParameters(self):\n","        \"\"\"\n","        Returns a list with gradients with respect to its parameters.\n","        If the module does not have parameters return empty list.\n","        \"\"\"\n","        return []\n","\n","    # Возвращает градиенты параметров.\n","    # Если у слоя нет параметров, возвращает [].\n","\n","    def train(self):\n","        \"\"\"\n","        Sets training mode for the module.\n","        Training and testing behaviour differs for Dropout, BatchNorm.\n","        \"\"\"\n","        self.training = True\n","\n","        # Устанавливает флаг training = True (режим обучения).\n","        # Когда нужно включать train()?\n","        # Перед началом обучения сети (например, перед вызовом forward и backward).\n","        # Некоторые слои (например, Dropout и BatchNorm) ведут себя по-разному в обучении и на тесте, поэтому режим training важен.\n","\n","    def evaluate(self):\n","        \"\"\"\n","        Sets evaluation mode for the module.\n","        Training and testing behaviour differs for Dropout, BatchNorm.\n","        \"\"\"\n","        self.training = False\n","\n","        # Устанавливает флаг training = False (режим тестирования).\n","        # Полезно для BatchNorm, Dropout.\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Pretty printing. Should be overrided in every module if you want\n","        to have readable description.\n","        \"\"\"\n","        return \"Module\"\n","\n","        # Определяет строковое представление класса.\n","        # В дочерних классах лучше переопределить:"]},{"cell_type":"markdown","metadata":{"id":"mKRkIjT8j6eZ"},"source":["# Sequential container"]},{"cell_type":"markdown","metadata":{"id":"Cb98PPpJj6ea"},"source":["**Define** a forward and backward pass procedures. \\\n","контейнер, который позволяет последовательно добавлять слои к модели, что делает процесс построения нейронной сети более интуитивным и простым."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":184,"status":"ok","timestamp":1743336205045,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"7y2lav4dj6ea"},"outputs":[],"source":["import torch.nn as nn\n","\n","class Sequential(nn.Module):\n","    def __init__(self):\n","        super(Sequential, self).__init__()\n","        self.modules = []\n","\n","    def add(self, module):\n","        \"\"\"Добавляет модуль (слой) в контейнер.\"\"\"\n","        self.modules.append(module)\n","\n","    def updateOutput(self, input):\n","        \"\"\"Прямой проход (forward pass) через все слои в контейнере.\"\"\"\n","        self.y = [input]  # Сохраняем входной тензор\n","        for module in self.modules:\n","            self.y.append(module.forward(self.y[-1]))  # Пропускаем через слои\n","        self.output = self.y[-1]  # Выход последнего слоя\n","        return self.output\n","\n","    def backward(self, input, gradOutput):\n","        \"\"\"Обратный проход (backward pass) через все слои.\"\"\"\n","        n = len(self.modules)\n","        grad = gradOutput  # Начальный градиент\n","\n","        for i in range(n - 1, -1, -1):  # Обратный проход\n","            grad = self.modules[i].backward(self.y[i], grad)\n","\n","        self.gradInput = grad\n","        return self.gradInput\n","\n","    def zeroGradParameters(self):\n","        \"\"\"Обнуляет градиенты во всех слоях.\"\"\"\n","        for module in self.modules:\n","            module.zeroGradParameters()\n","\n","    def getParameters(self):\n","        \"\"\"Собирает все параметры модели в список.\"\"\"\n","        return [module.getParameters() for module in self.modules]\n","\n","    def getGradParameters(self):\n","        \"\"\"Собирает все градиенты модели в список.\"\"\"\n","        return [module.getGradParameters() for module in self.modules]\n","\n","    def __repr__(self):\n","        return \"\\n\".join([str(module) for module in self.modules])\n","\n","    def __getitem__(self, x):\n","        return self.modules[x]\n","\n","    def train(self):\n","        \"\"\"Переключает все модули в режим обучения.\"\"\"\n","        self.training = True\n","        for module in self.modules:\n","            module.train()\n","\n","    def evaluate(self):\n","        \"\"\"Переключает все модули в режим оценки.\"\"\"\n","        self.training = False\n","        for module in self.modules:\n","            module.evaluate()\n"]},{"cell_type":"markdown","metadata":{"id":"zfXdYfO4j6ea"},"source":["# Layers"]},{"cell_type":"markdown","metadata":{"id":"ZuwvBkuNj6ea"},"source":["## 1 (0.2). Linear transform layer\n","Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n","- input:   **`batch_size x n_feats1`**\n","- output: **`batch_size x n_feats2`**"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1743336205047,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"D0uoyqkpj6ea"},"outputs":[],"source":["class Linear(Module):\n","    \"\"\"\n","    A module which applies a linear transformation\n","    A common name is fully-connected layer, InnerProductLayer in caffe.\n","\n","    The module should work with 2D input of shape (n_samples, n_feature).\n","    \"\"\"\n","    def __init__(self, n_in, n_out):\n","        super(Linear, self).__init__()\n","\n","        # This is a nice initialization\n","        # Инициализирует веса (self.W) и смещения (self.b) для линейного слоя.\n","        # Веса инициализируются случайно в диапазоне от -stdv до stdv, где stdv = 1 / sqrt(n_in). Это помогает предотвратить взрыв градиентов во время обучения.\n","        # Градиенты весов (self.gradW) и смещений (self.gradb) инициализируются нулями.\n","        stdv = 1./np.sqrt(n_in)\n","        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n","        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n","\n","        self.gradW = np.zeros_like(self.W)\n","        self.gradb = np.zeros_like(self.b)\n","\n","    # Применяет линейную трансформацию к входным данным.\n","    # Выход рассчитывается как матричное произведение входных данных и весов плюс смещение.\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        # self.output = ...\n","        self.output = input @ self.W.T + self.b\n","        return self.output\n","\n","    # Рассчитывает градиент ошибки по отношению к входным данным.\n","    # Используется для обратного распространения ошибки.\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        # self.gradInput = ...\n","        self.gradInput = gradOutput @ self.W\n","        return self.gradInput\n","\n","    # Рассчитывает и накапливает градиенты весов и смещений на основе градиента ошибки.\n","    def accGradParameters(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        # self.gradW = ... ; self.gradb = ...\n","        self.gradW = gradOutput.T @ input\n","        self.gradb = gradOutput.sum(axis = 0)\n","        pass\n","\n","    # Сбрасывает градиенты весов и смещений в ноль перед новой итерацией обучения.\n","    def zeroGradParameters(self):\n","        self.gradW.fill(0)\n","        self.gradb.fill(0)\n","\n","    def getParameters(self):\n","        return [self.W, self.b]\n","\n","    def getGradParameters(self):\n","        return [self.gradW, self.gradb]\n","\n","    # Когда вы вызываете функцию print() или repr() для объекта этого класса, будет выведена строка, например:\n","    # 'Linear 784 -> 10', что означает, что слой принимает 784 входных признака и выдает 10 выходных.\n","    def __repr__(self):\n","        s = self.W.shape\n","        q = 'Linear %d -> %d' %(s[1],s[0])\n","        return q"]},{"cell_type":"markdown","metadata":{"id":"tNOnHXZJj6eb"},"source":["## 2. (0.2) SoftMax\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n","\n","Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument. \\\n","Нормализация через константу: Вычитание максимума (x - max(x)) перед вычислением экспоненты предотвращает численную нестабильность"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743336205047,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"VIValI0hj6eb"},"outputs":[],"source":["class SoftMax(Module):\n","    def __init__(self):\n","         super(SoftMax, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n","\n","        # Your code goes here. ################################################\n","        self.output = np.exp(self.output)\n","        self.output = self.output / np.sum(self.output, axis=1, keepdims=True)\n","\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        self.gradInput = self.output * (gradOutput - np.sum(self.output * gradOutput, axis=1, keepdims=True))\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"SoftMax\""]},{"cell_type":"markdown","metadata":{"id":"Cy3DJjynj6eb"},"source":["## 3. (0.2) LogSoftMax\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n","\n","The main goal of this layer is to be used in computation of log-likelihood loss."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":66,"status":"ok","timestamp":1743336205114,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"Xo7DRdAJj6eb"},"outputs":[],"source":["class LogSoftMax(Module):\n","    def __init__(self):\n","         super(LogSoftMax, self).__init__()\n","\n","    def updateOutput(self, input):\n","        # start with normalization for numerical stability\n","        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n","\n","        # Your code goes here. ################################################\n","        self.output = self.output - np.log(np.sum(np.exp(self.output), axis = 1, keepdims = True))\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        self.gradInput = gradOutput - np.exp(self.output) * np.sum(gradOutput, axis = 1, keepdims = True)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"LogSoftMax\""]},{"cell_type":"markdown","metadata":{"id":"QP5QdmmPj6eb"},"source":["## 4. (0.3) Batch normalization\n","One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below. \\\n","\\\n","Одна из самых значительных идей, повлиявших на нейронные сети — Batch Normalization. Идея проста, но эффективна: признаки должны быть отбелены (среднее = 0, std = 1) на всех слоях нейросети. Это улучшает сходимость глубоких моделей, сокращая время обучения с недель до дней.\n","Ваша задача: реализовать первую часть слоя (нормализацию признаков). Вторая часть (ChannelwiseScaling) уже реализована.\n","\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\n","The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n","Слой Batch Normalization работает следующим образом:\n","В режиме обучения (self.training == True):\n","Преобразует входные данные по формуле \\\n","where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance:\\\n","Обновляет экспоненциальные скользящие средние для среднего и дисперсии:\n","```\n","    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n","    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n","```\n","During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \\\n","В режиме тестирования (self.training == False): \\\n","Нормализует входные данные, используя накопленные скользящие средние (moving_mean и moving_variance) вместо статистик текущего батча.\n","\n","Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":76,"status":"ok","timestamp":1743336205191,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"fGTTDqVgj6eb"},"outputs":[],"source":["class BatchNormalization(Module):\n","    EPS = 1e-3\n","    def __init__(self, alpha = 0.):\n","        super(BatchNormalization, self).__init__()\n","        self.alpha = alpha\n","        self.moving_mean = None\n","        self.moving_variance = None\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        # use self.EPS please\n","        if self.training == True:\n","            self.batch_mean = input.mean(axis = 0)\n","            self.batch_variance = input.var(axis = 0)\n","            self.output = (input - self.batch_mean) / (self.batch_variance + self.EPS)**0.5\n","\n","            self.moving_mean = self.moving_mean * self.alpha + self.batch_mean * (1 - self.alpha)\n","            self.moving_variance = self.moving_variance * self.alpha + self.batch_variance * (1 - self.alpha)\n","        else:\n","            self.output = (input - self.moving_mean) / (self.moving_variance + self.EPS)**0.5\n","\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        m = gradOutput.shape[0]\n","        self.gradInput = (m * gradOutput - np.sum(gradOutput, axis = 0) - self.output * np.sum(gradOutput*self.output, axis = 0))/(m*np.sqrt(self.batch_variance + self.EPS))\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"BatchNormalization\""]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1743336205203,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"8XUS3Lt-j6eb"},"outputs":[],"source":["# Не трогать этот класс\n","\n","class ChannelwiseScaling(Module):\n","    \"\"\"\n","       Implements linear transform of input y = \\gamma * x + \\beta\n","       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n","    \"\"\"\n","    def __init__(self, n_out):\n","        super(ChannelwiseScaling, self).__init__()\n","\n","        stdv = 1./np.sqrt(n_out)\n","        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n","        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n","\n","        self.gradGamma = np.zeros_like(self.gamma)\n","        self.gradBeta = np.zeros_like(self.beta)\n","\n","    def updateOutput(self, input):\n","        self.output = input * self.gamma + self.beta\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = gradOutput * self.gamma\n","        return self.gradInput\n","\n","    def accGradParameters(self, input, gradOutput):\n","        self.gradBeta = np.sum(gradOutput, axis=0)\n","        self.gradGamma = np.sum(gradOutput*input, axis=0)\n","\n","    def zeroGradParameters(self):\n","        self.gradGamma.fill(0)\n","        self.gradBeta.fill(0)\n","\n","    def getParameters(self):\n","        return [self.gamma, self.beta]\n","\n","    def getGradParameters(self):\n","        return [self.gradGamma, self.gradBeta]\n","\n","    def __repr__(self):\n","        return \"ChannelwiseScaling\""]},{"cell_type":"markdown","metadata":{"id":"vA5zjM3jj6eb"},"source":["Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer.\n","\n","Практические замечания: \\\n","Если BatchNormalization размещается после линейного преобразования (включая полносвязный слой, сверточные слои, поканальное масштабирование), которое реализует функцию вида: \\\n","\\\n","y = weight * x + bias \\\n","\\\n","то добавление смещения (bias) становится бесполезным и может быть опущено, так как его эффект будет уничтожен при вычитании среднего значения в Batch Normalization. \\\n","\\\n","Если BatchNormalization (за которым следует ChannelwiseScaling) размещается перед слоем, который сохраняет масштаб (например, ReLU, LeakyReLU), а затем следует любой линейный слой, то параметр gamma в ChannelwiseScaling` можно зафиксировать (заморозить), так как он может быть поглощен (перенесен) в линейный слой."]},{"cell_type":"markdown","metadata":{"id":"Gackeo1cj6eb"},"source":["## 5. (0.3) Dropout\n","Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n","\n","This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n","\n","While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n","\n","- input:   **`batch_size x n_feats`**\n","- output: **`batch_size x n_feats`**\n","\\\n","Идея и реализация очень просты: просто умножьте вход на маску Bernoulli(p). Здесь p — вероятность того, что элемент будет обнулен.\\\n","\\\n","Доказано, что это эффективный метод регуляризации и предотвращения коадаптации нейронов.\\\n","\\\n","Во время обучения (self.training == True) он должен выбирать маску на каждой итерации (для каждого батча), обнулять элементы и умножать элементы на $1 / (1 - p)$. Последнее необходимо для поддержания средних значений признаков близкими к средним значениям, которые будут в тестовом режиме. При тестировании этот модуль должен реализовать тождественное преобразование, то есть self.output = input."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1743336205215,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"NmLQV3jXj6eb"},"outputs":[],"source":["class Dropout(Module):\n","    def __init__(self, p=0.5):\n","        super(Dropout, self).__init__()\n","\n","        self.p = p\n","        self.mask = None\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        self.mask = np.random.binomial(1, 1. - self.p, input.shape)\n","        if self.training == True:\n","            self.output = input * self.mask\n","            self.output = self.output / (1 - self.p)\n","        else:\n","            self.output = input\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        if self.training == True:\n","            self.gradInput = gradOutput * self.mask\n","            self.gradInput = self.gradInput / (1 - self.p)\n","        else:\n","            self.gradInput = gradOutput\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"Dropout\""]},{"cell_type":"markdown","metadata":{"id":"-WHGIqJFlhz2"},"source":["# 6. (2.0) Conv2d\n","Implement [**Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). Use only this list of parameters: (in_channels, out_channels, kernel_size, stride, padding, bias, padding_mode) and fix dilation=1 and groups=1."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1743336205229,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"VejA6-i2IcJX"},"outputs":[],"source":["class Conv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size,\n","                 stride=1, padding=0, bias=True, padding_mode='zeros'):\n","        super(Conv2d, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.bias_flag = bias\n","        self.padding_mode = padding_mode\n","        self.dilation = 1\n","        self.groups = 1\n","\n","        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.01)\n","        if bias:\n","            self.bias = nn.Parameter(torch.randn(out_channels) * 0.01)\n","        else:\n","            self.bias = None\n","\n","    def updateOutput(self, input):\n","        \"\"\" Вычисляет выход сверточного слоя \"\"\"\n","        if isinstance(input, np.ndarray):\n","            input = torch.tensor(input, dtype=torch.float32)\n","\n","        if self.padding_mode != 'zeros':\n","            pad_h, pad_w = (self.padding, self.padding) if isinstance(self.padding, int) else self.padding\n","            input = F.pad(input, (pad_w, pad_w, pad_h, pad_h), mode=self.padding_mode)\n","            padding = 0  # Padding уже применили вручную\n","        else:\n","            padding = self.padding\n","\n","        self.output = F.conv2d(input, self.weight, self.bias, stride=self.stride, padding=padding, dilation=self.dilation, groups=self.groups)\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        \"\"\" Вычисляет градиент по входу \"\"\"\n","        if isinstance(input, np.ndarray):\n","            input = torch.tensor(input, dtype=torch.float32)\n","\n","        self.gradInput = torch.autograd.grad(\n","            outputs=self.output, inputs=input, grad_outputs=gradOutput,\n","            only_inputs=True, retain_graph=True\n","        )[0]\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return (f\"Conv2d({self.in_channels}, {self.out_channels}, kernel_size={self.kernel_size}, \"\n","                f\"stride={self.stride}, padding={self.padding}, bias={self.bias_flag}, \"\n","                f\"padding_mode={self.padding_mode})\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743336205240,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"wGOISZ5wIcJX"},"outputs":[],"source":["# # Тестирование класса\n","# params_list = [\n","#     {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n","#     {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 28, 'width': 28, 'kernel_size': 5, 'stride': 2, 'padding': 2, 'bias': False, 'padding_mode': 'replicate'},\n","#     {'batch_size': 16, 'in_channels': 3, 'out_channels': 3, 'height': 64, 'width': 64, 'kernel_size': 3, 'stride': 2, 'padding': 'same', 'bias': True, 'padding_mode': 'reflect'},\n","#     {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10, 'kernel_size': 2, 'stride': (1, 2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n","# ]\n","\n","# for params in params_list:\n","#     print(f\"Testing Conv2d with params: {params}\")\n","#     conv = Conv2d(\n","#         params['in_channels'], params['out_channels'], params['kernel_size'],\n","#         stride=params['stride'], padding=params['padding'],\n","#         bias=params['bias'], padding_mode=params['padding_mode']\n","#     )\n","#     x = torch.randn(params['batch_size'], params['in_channels'], params['height'], params['width'])\n","#     output = conv.updateOutput(x)\n","#     print(f\"Output shape: {output.shape}\\n\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":50,"status":"ok","timestamp":1743336205291,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"UYbU9KO_IcJX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"updUVZE9qixP"},"source":["# 7. (0.5) Implement [**MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [**AvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html).\n","\n","Use only parameters like kernel_size, stride, padding (negative infinity for maxpool and zero for avgpool) and other parameters fixed as in framework."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743336205292,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"Qys58EzkqhLj"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Module\n","import numpy as np\n","\n","class MaxPool2d(Module):\n","    def __init__(self, kernel_size, stride, padding):\n","        super(MaxPool2d, self).__init__()\n","\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","\n","    def updateOutput(self, input):\n","        if isinstance(input, np.ndarray):\n","            input = torch.tensor(input, dtype=torch.float32, requires_grad=True)\n","\n","        self.output, self.indices = F.max_pool2d(\n","            input=input,\n","            kernel_size=self.kernel_size,\n","            stride=self.stride,\n","            padding=self.padding,\n","            return_indices=True\n","        )\n","        return self.output.detach().numpy()\n","\n","    def updateGradInput(self, input, gradOutput):\n","        if isinstance(input, np.ndarray):\n","            input = torch.tensor(input, dtype=torch.float32, requires_grad=True)\n","        if isinstance(gradOutput, np.ndarray):\n","            gradOutput = torch.tensor(gradOutput, dtype=torch.float32)\n","\n","        self.gradInput = F.max_unpool2d(\n","            gradOutput,\n","            self.indices,\n","            kernel_size=self.kernel_size,\n","            stride=self.stride,\n","            padding=self.padding\n","        )\n","        return self.gradInput.detach().numpy()\n","\n","    def __repr__(self):\n","        return f\"MaxPool2d(kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})\"\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1743336205293,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"d23uxZxoIcJX"},"outputs":[],"source":["class AvgPool2d(Module):\n","    def __init__(self, kernel_size, stride, padding):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","\n","    def updateOutput(self, input):\n","        self.input = torch.tensor(input, dtype=torch.float32, requires_grad=True) if isinstance(input, np.ndarray) else input\n","\n","        self.output = F.avg_pool2d(\n","            self.input,\n","            kernel_size=self.kernel_size,\n","            stride=self.stride,\n","            padding=self.padding\n","        )\n","        return self.output.detach().numpy()\n","\n","    def updateGradInput(self, input, gradOutput):\n","        grad_output_tensor = torch.tensor(gradOutput) if isinstance(gradOutput, np.ndarray) else gradOutput\n","\n","        self.output.backward(gradient=grad_output_tensor)\n","        self.gradInput = self.input.grad.numpy()\n","\n","        return self.gradInput\n"]},{"cell_type":"markdown","metadata":{"id":"KTN5R3CwrukV"},"source":["# 8. (0.3) Implement **GlobalMaxPool2d** and **GlobalAvgPool2d**.\n","They do not have testing and parameters are up to you but they must aggregate information within channels. Write test functions for these layers on your own."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":59,"status":"ok","timestamp":1743336205350,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"1oNQeDfiRIHC"},"outputs":[],"source":["class GlobalMaxPool2d(Module):\n","    def __init__(self):\n","        super(GlobalMaxPool2d, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = F.adaptive_max_pool2d(input, (1, 1))\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = F.interpolate(\n","            gradOutput, size=input.shape[2:], mode='nearest'\n","        )\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"GlobalMaxPool2d\"\n","\n","\n","class GlobalAvgPool2d(Module):\n","    def __init__(self):\n","        super(GlobalAvgPool2d, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = F.adaptive_avg_pool2d(input, (1, 1))\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = F.interpolate(\n","            gradOutput, size=input.shape[2:], mode='nearest'\n","        )\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"GlobalAvgPool2d\"\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1743336205375,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"TqVj9f2oIcJY","outputId":"fa65e6a5-6bd6-403d-bce7-58e9065cd49b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing GlobalMaxPool2d with params: {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32}\n","GlobalMaxPool2d test passed for params: {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32}\n","\n","Testing GlobalMaxPool2d with params: {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28}\n","GlobalMaxPool2d test passed for params: {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28}\n","\n","Testing GlobalMaxPool2d with params: {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64}\n","GlobalMaxPool2d test passed for params: {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64}\n","\n","Testing GlobalMaxPool2d with params: {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10}\n","GlobalMaxPool2d test passed for params: {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10}\n","\n","Testing GlobalAvgPool2d with params: {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32}\n","GlobalAvgPool2d test passed for params: {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32}\n","\n","Testing GlobalAvgPool2d with params: {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28}\n","GlobalAvgPool2d test passed for params: {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28}\n","\n","Testing GlobalAvgPool2d with params: {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64}\n","GlobalAvgPool2d test passed for params: {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64}\n","\n","Testing GlobalAvgPool2d with params: {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10}\n","GlobalAvgPool2d test passed for params: {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10}\n","\n"]}],"source":["def test_GlobalMaxPool2d():\n","    params_list = [\n","        {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32},\n","        {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28},\n","        {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64},\n","        {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10},\n","    ]\n","\n","    for params in params_list:\n","        print(f\"Testing GlobalMaxPool2d with params: {params}\")\n","        pool = GlobalMaxPool2d()\n","        x = torch.randn(params['batch_size'], params['channels'], params['height'], params['width'], requires_grad=True)\n","        output = pool.updateOutput(x)\n","\n","        assert output.shape == (params['batch_size'], params['channels'], 1, 1), \"GlobalMaxPool2d: неверная форма выхода\"\n","\n","        expected = F.adaptive_max_pool2d(x, (1, 1))\n","        assert torch.allclose(output, expected), \"GlobalMaxPool2d: ошибка значений\"\n","\n","        # Проверка backward\n","        grad_output = torch.ones_like(output)\n","        grad_input = pool.updateGradInput(x, grad_output)\n","        grad_input_expected = F.interpolate(grad_output, size=x.shape[2:], mode='nearest')\n","        assert torch.allclose(grad_input, grad_input_expected), \"GlobalMaxPool2d: ошибка backward\"\n","\n","        print(f\"GlobalMaxPool2d test passed for params: {params}\\n\")\n","\n","\n","def test_GlobalAvgPool2d():\n","    params_list = [\n","        {'batch_size': 8, 'channels': 3, 'height': 32, 'width': 32},\n","        {'batch_size': 4, 'channels': 1, 'height': 28, 'width': 28},\n","        {'batch_size': 16, 'channels': 3, 'height': 64, 'width': 64},\n","        {'batch_size': 2, 'channels': 3, 'height': 10, 'width': 10},\n","    ]\n","\n","    for params in params_list:\n","        print(f\"Testing GlobalAvgPool2d with params: {params}\")\n","        pool = GlobalAvgPool2d()\n","        x = torch.randn(params['batch_size'], params['channels'], params['height'], params['width'], requires_grad=True)\n","        output = pool.updateOutput(x)\n","\n","        assert output.shape == (params['batch_size'], params['channels'], 1, 1), \"GlobalAvgPool2d: неверная форма выхода\"\n","\n","        expected = F.adaptive_avg_pool2d(x, (1, 1))\n","        assert torch.allclose(output, expected), \"GlobalAvgPool2d: ошибка значений\"\n","\n","        grad_output = torch.ones_like(output)\n","        grad_input = pool.updateGradInput(x, grad_output)\n","        grad_input_expected = F.interpolate(grad_output, size=x.shape[2:], mode='nearest')\n","        assert torch.allclose(grad_input, grad_input_expected), \"GlobalAvgPool2d: ошибка backward\"\n","\n","        print(f\"GlobalAvgPool2d test passed for params: {params}\\n\")\n","\n","\n","# Запуск тестов\n","test_GlobalMaxPool2d()\n","test_GlobalAvgPool2d()"]},{"cell_type":"markdown","metadata":{"id":"cYeBQDBhtViy"},"source":["# 9. (0.2) Implement [**Flatten**](https://pytorch.org/docs/stable/generated/torch.flatten.html)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1743336205389,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"SimPEMOFqhTQ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class Flatten(nn.Module):\n","    def __init__(self, start_dim=0, end_dim=-1):\n","        super(Flatten, self).__init__()\n","        self.start_dim = start_dim\n","        self.end_dim = end_dim\n","\n","    def updateOutput(self, input):\n","        if not isinstance(input, torch.Tensor):\n","            input = torch.tensor(input, dtype=torch.float32)\n","\n","        self.output = torch.flatten(input, start_dim=self.start_dim, end_dim=self.end_dim)\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        if not isinstance(input, torch.Tensor):\n","            input = torch.tensor(input, dtype=torch.float32)\n","        if not isinstance(gradOutput, torch.Tensor):\n","            gradOutput = torch.tensor(gradOutput, dtype=torch.float32)\n","\n","        self.gradInput = gradOutput.view(input.shape)  # Восстанавливаем изначальную форму\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"Flatten\"\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1743336205390,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"nbv5Um7zIcJY"},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"o36vPHSSj6eb"},"source":["# Activation functions"]},{"cell_type":"markdown","metadata":{"id":"l_pryRQIj6ec"},"source":["Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**):"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743336205398,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"sgm8bXjKj6ec"},"outputs":[],"source":["class ReLU(Module):\n","    def __init__(self):\n","         super(ReLU, self).__init__()\n","\n","    def updateOutput(self, input):\n","        self.output = np.maximum(input, 0)\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        self.gradInput = np.multiply(gradOutput , input > 0)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ReLU\""]},{"cell_type":"markdown","metadata":{"id":"yB0UHGagj6ec"},"source":["## 10. (0.1) Leaky ReLU\n","Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope."]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1743336205408,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"agwfkwO0j6ec"},"outputs":[],"source":["class LeakyReLU(Module):\n","    def __init__(self, slope = 0.03):\n","        super(LeakyReLU, self).__init__()\n","\n","        self.slope = slope\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        self.output = np.maximum(input, 0) + np.minimum(self.slope * input, 0)\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        self.gradInput = np.multiply(gradOutput, input > 0) + self.slope * np.multiply(gradOutput, input < 0)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"LeakyReLU\""]},{"cell_type":"markdown","metadata":{"id":"t-STyecvj6ec"},"source":["## 11. (0.1) ELU\n","Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":80,"status":"ok","timestamp":1743336205501,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"jJSzEu1mj6ec"},"outputs":[],"source":["class ELU(Module):\n","    def __init__(self, alpha = 1.0):\n","        super(ELU, self).__init__()\n","\n","        self.alpha = alpha\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        self.output = np.maximum(input, 0) + np.minimum(self.alpha * (np.exp(input) - 1), 0)\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        self.gradInput = np.multiply(gradOutput, input > 0) + self.alpha * np.exp(input) * np.multiply(gradOutput, input < 0)\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ELU\""]},{"cell_type":"markdown","metadata":{"id":"Gn3C7KTqj6ec"},"source":["## 12. (0.1) SoftPlus\n","Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1743336205502,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"xcDPMssrj6ec"},"outputs":[],"source":["# Плавное приближение RELU\n","class SoftPlus(Module):\n","    def __init__(self):\n","        super(SoftPlus, self).__init__()\n","\n","    def updateOutput(self, input):\n","        # Your code goes here. ################################################\n","        self.output = np.log(1 + np.exp(input))\n","        return  self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        # Your code goes here. ################################################\n","        self.gradInput = gradOutput / (1 + np.exp(-input))\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"SoftPlus\""]},{"cell_type":"markdown","metadata":{"id":"kw3PeZjOuo0e"},"source":["## 13. (0.2) Gelu\n","Implement [**Gelu**](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) activations."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1743336205549,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"SdieE0Dtuo8j"},"outputs":[],"source":["class Gelu(nn.Module):\n","    def __init__(self):\n","        super(Gelu, self).__init__()\n","\n","    def updateOutput(self, input):\n","        if not isinstance(input, torch.Tensor):\n","            input = torch.tensor(input, dtype=torch.float32)\n","\n","        sqrt_2 = torch.sqrt(torch.tensor(2.0, dtype=input.dtype, device=input.device))\n","        self.output = input * 0.5 * (1 + torch.erf(input / sqrt_2))\n","        return self.output\n","\n","    def updateGradInput(self, input, gradOutput):\n","        if not isinstance(input, torch.Tensor):\n","            input = torch.tensor(input, dtype=torch.float32)\n","        if not isinstance(gradOutput, torch.Tensor):\n","            gradOutput = torch.tensor(gradOutput, dtype=torch.float32)\n","\n","        sqrt_2pi = torch.sqrt(torch.tensor(2.0 * torch.pi, dtype=input.dtype, device=input.device))\n","        exp_term = torch.exp(-0.5 * input**2)\n","        cdf = 0.5 * (1 + torch.erf(input / torch.sqrt(torch.tensor(2.0, dtype=input.dtype, device=input.device))))\n","        pdf = exp_term / sqrt_2pi\n","\n","        derivative = cdf + input * pdf\n","        self.gradInput = derivative * gradOutput\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"Gelu\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"55p7UvPAj6ec"},"source":["# Criterions"]},{"cell_type":"markdown","metadata":{"id":"5NFaxZaqj6ec"},"source":["Criterions are used to score the models answers."]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1743336205579,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"XGu45A8qj6ec"},"outputs":[],"source":["class Criterion(object):\n","    def __init__ (self):\n","        self.output = None\n","        self.gradInput = None\n","\n","    def forward(self, input, target):\n","        \"\"\"\n","            Given an input and a target, compute the loss function\n","            associated to the criterion and return the result.\n","\n","            For consistency this function should not be overrided,\n","            all the code goes in `updateOutput`.\n","        \"\"\"\n","        return self.updateOutput(input, target)\n","\n","    def backward(self, input, target):\n","        \"\"\"\n","            Given an input and a target, compute the gradients of the loss function\n","            associated to the criterion and return the result.\n","\n","            For consistency this function should not be overrided,\n","            all the code goes in `updateGradInput`.\n","        \"\"\"\n","        return self.updateGradInput(input, target)\n","\n","    def updateOutput(self, input, target):\n","        \"\"\"\n","        Function to override.\n","        Метод, который необходимо переопределить в дочерних классах для реализации конкретной функции потерь (например, среднеквадратичная ошибка,\n","        перекрестная энтропия). Должен вычислить значение функции потерь и сохранить его в self.output.\n","        \"\"\"\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        \"\"\"\n","        Function to override.\n","        Метод, который необходимо переопределить в дочерних классах для вычисления градиентов функции потерь\n","        по отношению к входным данным. Должен вычислить градиенты и сохранить их в self.gradInput\n","        \"\"\"\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        \"\"\"\n","        Pretty printing. Should be overrided in every module if you want\n","        to have readable description.\n","        \"\"\"\n","        return \"Criterion\""]},{"cell_type":"markdown","metadata":{"id":"WuU26xkpj6ec"},"source":["The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n","- input:   **`batch_size x n_feats`**\n","- target: **`batch_size x n_feats`**\n","- output: **scalar**"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1743336205597,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"-i3VNuHhj6ec"},"outputs":[],"source":["class MSECriterion(Criterion):\n","    def __init__(self):\n","        super(MSECriterion, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        self.gradInput  = (input - target) * 2 / input.shape[0]\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"MSECriterion\""]},{"cell_type":"markdown","metadata":{"id":"x8LKLWNVj6ec"},"source":["## 14. (0.2) Negative LogLikelihood criterion (numerically unstable)\n","You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula,\n","remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n","- input:   **`batch_size x n_feats`** - probabilities\n","- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n","- output: **scalar**\n","\n","Ваша задача - реализовать класс ClassNLLCriterion. Он должен реализовывать мультиклассовую логарифмическую потерю. Несмотря на то, что в этой формуле есть сумма по y (целевой переменной), помните, что целевые значения представлены в формате one-hot encoding. Этот факт значительно упрощает вычисления. Обратите внимание, что критерии - это единственные места, где вы делите на размер пакета (batch size). Также, есть небольшой хак с добавлением малого числа к вероятностям, чтобы избежать вычисления логарифма от 0 (log(0)).\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":59,"status":"ok","timestamp":1743336205651,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"die7KvW6j6ec"},"outputs":[],"source":["class ClassNLLCriterionUnstable(Criterion):\n","    EPS = 1e-15\n","    def __init__(self):\n","        a = super(ClassNLLCriterionUnstable, self)\n","        super(ClassNLLCriterionUnstable, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","\n","        # Use this trick to avoid numerical errors\n","        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n","\n","        # Your code goes here. ################################################\n","        self.n = len(input)\n","\n","        self.output = (-1) * np.sum(target * np.log(input_clamp)) / self.n\n","\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","\n","        # Use this trick to avoid numerical errors\n","        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n","\n","        # Your code goes here. ################################################\n","        self.gradInput = (-1) * target / input_clamp / self.n\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ClassNLLCriterionUnstable\""]},{"cell_type":"markdown","metadata":{"id":"uHr_JbU5j6ec"},"source":["## 15. (0.3) Negative LogLikelihood criterion (numerically stable)\n","- input:   **`batch_size x n_feats`** - log probabilities\n","- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n","- output: **scalar**\n","\n","Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1743336205653,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"cEFfpoa4IcJe"},"outputs":[],"source":["class ClassNLLCriterion(Criterion):\n","    def __init__(self):\n","        a = super(ClassNLLCriterion, self)\n","        super(ClassNLLCriterion, self).__init__()\n","\n","    def updateOutput(self, input, target):\n","        # Your code goes here. ################################################\n","        self.n = len(input)\n","        self.output = (-1) * np.sum(target * (input + 1e-15))/ self.n\n","        return self.output\n","\n","    def updateGradInput(self, input, target):\n","        # Your code goes here. ################################################\n","        self.gradInput = (-1) * target / self.n\n","        return self.gradInput\n","\n","    def __repr__(self):\n","        return \"ClassNLLCriterion\""]},{"cell_type":"markdown","metadata":{"id":"TC2Bf1PP2Ios"},"source":["1-я часть задания: реализация слоев, лосей и функций активации - 5 баллов. \\\n","2-я часть задания: реализация моделей на своих классах. Что должно быть:\n","  1. Выберите оптимизатор и реализуйте его, чтоб он работал с вами классами. - 1 балл.\n","  2. Модель для задачи мультирегрессии на выбраных вами данных. Использовать FCNN, dropout, batchnorm, MSE. Пробуйте различные фукнции активации. Для первой модели попробуйте большую, среднюю и маленькую модель. - 1 балл.\n","  3. Модель для задачи мультиклассификации на MNIST. Использовать свёртки, макспулы, флэттэны, софтмаксы - 1 балла.\n","  4. Автоэнкодер для выбранных вами данных. Должен быть на свёртках и полносвязных слоях, дропаутах, батчнормах и тд. - 2 балла. \\\\\n","\n","Дополнительно в оценке каждой модели будет учитываться:\n","1. Наличие правильно выбранной метрики и лосс функции.\n","2. Отрисовка графиков лосей и метрик на трейне-валидации. Проверка качества модели на тесте.\n","3. Наличие шедулера для lr.\n","4. Наличие вормапа.\n","5. Наличие механизма ранней остановки и сохранение лучшей модели.\n","6. Свитч лося (метрики) и оптимайзера."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6814,"status":"ok","timestamp":1743336212567,"user":{"displayName":"Роман Федосеев","userId":"07284688162535365380"},"user_tz":-180},"id":"nMej-1Y4If-3","outputId":"c89f8d11-f581-48f3-f3a1-c4e1eb327f1d"},"outputs":[{"output_type":"stream","name":"stderr","text":["test_AvgPool2d (__main__.TestLayers.test_AvgPool2d) ... ok\n","test_BatchNormalization (__main__.TestLayers.test_BatchNormalization) ... ok\n","test_ClassNLLCriterion (__main__.TestLayers.test_ClassNLLCriterion) ... ok\n","test_ClassNLLCriterionUnstable (__main__.TestLayers.test_ClassNLLCriterionUnstable) ... ok\n","test_Dropout (__main__.TestLayers.test_Dropout) ... ok\n","test_ELU (__main__.TestLayers.test_ELU) ... ok\n","test_Flatten (__main__.TestLayers.test_Flatten) ... ok\n","test_Gelu (__main__.TestLayers.test_Gelu) ... ok\n","test_LeakyReLU (__main__.TestLayers.test_LeakyReLU) ... ok\n","test_Linear (__main__.TestLayers.test_Linear) ... ok\n","test_LogSoftMax (__main__.TestLayers.test_LogSoftMax) ... ok\n","test_MaxPool2d (__main__.TestLayers.test_MaxPool2d) ... ok\n","test_Sequential (__main__.TestLayers.test_Sequential) ... ok\n","test_SoftMax (__main__.TestLayers.test_SoftMax) ... ok\n","test_SoftPlus (__main__.TestLayers.test_SoftPlus) ... ok\n","\n","----------------------------------------------------------------------\n","Ran 15 tests in 6.706s\n","\n","OK\n"]},{"output_type":"execute_result","data":{"text/plain":["<unittest.runner.TextTestResult run=15 errors=0 failures=0>"]},"metadata":{},"execution_count":26}],"source":["class TestLayers(unittest.TestCase):\n","    def test_Linear(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in, n_out = 2, 3, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.Linear(n_in, n_out)\n","            custom_layer = Linear(n_in, n_out)\n","            custom_layer.W = torch_layer.weight.data.numpy()\n","            custom_layer.b = torch_layer.bias.data.numpy()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","            # 3. check layer parameters grad\n","            custom_layer.accGradParameters(layer_input, next_layer_grad)\n","            weight_grad = custom_layer.gradW\n","            bias_grad = custom_layer.gradb\n","            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n","            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n","\n","    def test_SoftMax(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.Softmax(dim=1)\n","            custom_layer = SoftMax()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","            next_layer_grad = next_layer_grad.clip(1e-5,1.)\n","            next_layer_grad = 1. / next_layer_grad\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n","\n","    def test_LogSoftMax(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.LogSoftmax(dim=1)\n","            custom_layer = LogSoftMax()\n","\n","            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n","            next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_BatchNormalization(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 32, 16\n","        for _ in range(100):\n","            # layers initialization\n","            slope = np.random.uniform(0.01, 0.05)\n","            alpha = 0.9\n","            custom_layer = BatchNormalization(alpha)\n","            custom_layer.train()\n","            torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n","            custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n","\n","            # 3. check moving mean\n","            self.assertTrue(np.allclose(custom_layer.moving_mean, torch_layer.running_mean.numpy()))\n","            # we don't check moving_variance because pytorch uses slightly different formula for it:\n","            # it computes moving average for unbiased variance (i.e var*N/(N-1))\n","            #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n","\n","            # 4. check evaluation mode\n","            custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","            custom_layer.evaluate()\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            torch_layer.eval()\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","    def test_Sequential(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            alpha = 0.9\n","            torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n","            torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n","            custom_layer = Sequential()\n","            bn_layer = BatchNormalization(alpha)\n","            bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n","            bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n","            custom_layer.add(bn_layer)\n","            scaling_layer = ChannelwiseScaling(n_in)\n","            scaling_layer.gamma = torch_layer.weight.data.numpy()\n","            scaling_layer.beta = torch_layer.bias.data.numpy()\n","            custom_layer.add(scaling_layer)\n","            custom_layer.train()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-5))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5))\n","\n","            # 3. check layer parameters grad\n","            weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n","            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n","            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n","            self.assertTrue(np.allclose(torch_weight_grad, weight_grad, atol=1e-6))\n","            self.assertTrue(np.allclose(torch_bias_grad, bias_grad, atol=1e-6))\n","\n","    def test_Dropout(self):\n","        np.random.seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            p = np.random.uniform(0.3, 0.7)\n","            layer = Dropout(p)\n","            layer.train()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0),\n","                                        np.isclose(layer_output*(1.-p), layer_input))))\n","\n","            # 2. check layer input grad\n","            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","            self.assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0),\n","                                        np.isclose(layer_grad*(1.-p), next_layer_grad))))\n","\n","            # 3. check evaluation mode\n","            layer.evaluate()\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.allclose(layer_output, layer_input))\n","\n","            # 4. check mask\n","            p = 0.0\n","            layer = Dropout(p)\n","            layer.train()\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.allclose(layer_output, layer_input))\n","\n","            p = 0.5\n","            layer = Dropout(p)\n","            layer.train()\n","            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            layer_output = layer.updateOutput(layer_input)\n","            zeroed_elem_mask = np.isclose(layer_output, 0)\n","            layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n","            self.assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n","\n","            # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n","            batch_size, n_in = 1000, 1\n","            p = 0.8\n","            layer = Dropout(p)\n","            layer.train()\n","\n","            layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n","\n","            layer_input = layer_input.T\n","            layer_output = layer.updateOutput(layer_input)\n","            self.assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n","\n","    # def test_Conv2d(self):\n","    #     hyperparams = [\n","    #         {'batch_size': 8, 'in_channels': 3, 'out_channels': 6, 'height': 32, 'width': 32,\n","    #          'kernel_size': 3, 'stride': 1, 'padding': 1, 'bias': True, 'padding_mode': 'zeros'},\n","    #         {'batch_size': 4, 'in_channels': 1, 'out_channels': 2, 'height': 28, 'width': 28,\n","    #          'kernel_size': 5, 'stride': 2, 'padding': 2, 'bias': False, 'padding_mode': 'replicate'},\n","    #         {'batch_size': 16, 'in_channels': 3, 'out_channels': 3, 'height': 64, 'width': 64,\n","    #          'kernel_size': 3, 'stride': 2, 'padding': 'same', 'bias': True, 'padding_mode': 'reflect'},\n","    #         {'batch_size': 2, 'in_channels': 3, 'out_channels': 8, 'height': 10, 'width': 10,\n","    #          'kernel_size': 2, 'stride': (1,2), 'padding': 0, 'bias': True, 'padding_mode': 'zeros'},\n","    #     ]\n","    #     np.random.seed(42)\n","    #     torch.manual_seed(42)\n","\n","    #     for _ in range(100):\n","    #       for params in hyperparams:\n","    #           with self.subTest(params=params):\n","\n","    #               batch_size = params['batch_size']\n","    #               in_channels = params['in_channels']\n","    #               out_channels = params['out_channels']\n","    #               height = params['height']\n","    #               width = params['width']\n","    #               kernel_size = params['kernel_size']\n","    #               stride = params['stride']\n","    #               padding = params['padding']\n","    #               bias = params['bias']\n","    #               padding_mode = params['padding_mode']\n","\n","    #               custom_layer = Conv2d(in_channels, out_channels, kernel_size,\n","    #                                     stride=stride, padding=padding, bias=bias,\n","    #                                     padding_mode=padding_mode)\n","    #               custom_layer.train()\n","\n","    #               torch_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n","    #                                             stride=stride, padding=padding, bias=bias,\n","    #                                             padding_mode=padding_mode)\n","\n","    #               custom_layer.weight = torch_layer.weight.detach().numpy().copy()\n","    #               if bias:\n","    #                   custom_layer.bias = torch_layer.bias.detach().numpy().copy()\n","\n","    #               layer_input = np.random.randn(batch_size, in_channels, height, width).astype(np.float32)\n","    #               input_var = torch.tensor(layer_input, requires_grad=True)\n","\n","    #               custom_output = custom_layer.updateOutput(layer_input)\n","    #               torch_output = torch_layer(input_var)\n","    #               self.assertTrue(\n","    #                   np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","    #               next_layer_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","    #               custom_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","    #               torch_output.backward(torch.tensor(next_layer_grad))\n","    #               torch_grad = input_var.grad.detach().numpy()\n","    #               self.assertTrue(\n","    #                   np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","\n","    def test_LeakyReLU(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            slope = np.random.uniform(0.01, 0.05)\n","            torch_layer = torch.nn.LeakyReLU(slope)\n","            custom_layer = LeakyReLU(slope)\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ELU(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            alpha = 1.0\n","            torch_layer = torch.nn.ELU(alpha)\n","            custom_layer = ELU(alpha)\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_SoftPlus(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.Softplus()\n","            custom_layer = SoftPlus()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var)\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n","            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ClassNLLCriterionUnstable(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.NLLLoss()\n","            custom_layer = ClassNLLCriterionUnstable()\n","\n","            layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n","            layer_input /= layer_input.sum(axis=-1, keepdims=True)\n","            layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n","            target_labels = np.random.choice(n_in, batch_size)\n","            target = np.zeros((batch_size, n_in), np.float32)\n","            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(torch.log(layer_input_var),\n","                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","            torch_layer_output_var.backward()\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","    def test_ClassNLLCriterion(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, n_in = 2, 4\n","        for _ in range(100):\n","            # layers initialization\n","            torch_layer = torch.nn.NLLLoss()\n","            custom_layer = ClassNLLCriterion()\n","\n","            layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n","            layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n","            target_labels = np.random.choice(n_in, batch_size)\n","            target = np.zeros((batch_size, n_in), np.float32)\n","            target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n","\n","            # 1. check layer output\n","            custom_layer_output = custom_layer.updateOutput(layer_input, target)\n","            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n","            torch_layer_output_var = torch_layer(layer_input_var,\n","                                                 Variable(torch.from_numpy(target_labels), requires_grad=False))\n","            self.assertTrue(np.allclose(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6))\n","\n","            # 2. check layer input grad\n","            custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n","            torch_layer_output_var.backward()\n","            torch_layer_grad_var = layer_input_var.grad\n","            self.assertTrue(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n","\n","\n","    def test_MaxPool2d(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, channels, height, width = 4, 3, 16, 16\n","        kernel_size, stride, padding = 2, 2, 0\n","\n","        for _ in range(100):\n","          custom_module = MaxPool2d(kernel_size, stride, padding)\n","          custom_module.train()\n","\n","          torch_module = torch.nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n","\n","          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","    def test_AvgPool2d(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        batch_size, channels, height, width = 4, 3, 16, 16\n","        kernel_size, stride, padding = 3, 2, 1\n","\n","        for _ in range(100):\n","          custom_module = AvgPool2d(kernel_size, stride, padding)\n","          custom_module.train()\n","\n","          torch_module = torch.nn.AvgPool2d(kernel_size, stride=stride, padding=padding)\n","\n","          input_np = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","    def test_Flatten(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        test_params = [\n","            {'start_dim': 1, 'end_dim': -1},\n","            {'start_dim': 2, 'end_dim': 3},\n","            {'start_dim': 0, 'end_dim': -1},\n","        ]\n","\n","        for _ in range(100):\n","          for params in test_params:\n","              with self.subTest(params=params):\n","                  start_dim = params['start_dim']\n","                  end_dim = params['end_dim']\n","\n","                  custom_module = Flatten(start_dim, end_dim)\n","                  input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)\n","                  input_var = torch.tensor(input_np, requires_grad=True)\n","\n","                  custom_output = custom_module.updateOutput(input_np)\n","                  torch_output = torch.flatten(input_var, start_dim=start_dim, end_dim=end_dim)\n","                  self.assertTrue(\n","                      np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","                  next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","                  custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","                  torch_output.backward(torch.tensor(next_grad))\n","                  torch_grad = input_var.grad.detach().numpy()\n","                  self.assertTrue(\n","                      np.allclose(torch_grad, custom_grad, atol=1e-6))\n","\n","    def test_Gelu(self):\n","        np.random.seed(42)\n","        torch.manual_seed(42)\n","\n","        for _ in range(100):\n","          custom_module = Gelu()\n","          custom_module.train()\n","\n","          torch_module = torch.nn.GELU()\n","\n","          input_np = np.random.randn(10, 5).astype(np.float32)\n","          input_var = torch.tensor(input_np, requires_grad=True)\n","\n","          custom_output = custom_module.updateOutput(input_np)\n","          torch_output = torch_module(input_var)\n","          self.assertTrue(\n","              np.allclose(torch_output.detach().numpy(), custom_output, atol=1e-6))\n","\n","          next_grad = np.random.randn(*torch_output.shape).astype(np.float32)\n","          custom_grad = custom_module.updateGradInput(input_np, next_grad)\n","          torch_output.backward(torch.tensor(next_grad))\n","          torch_grad = input_var.grad.detach().numpy()\n","          self.assertTrue(\n","              np.allclose(torch_grad, custom_grad, atol=1e-5))\n","\n","\n","suite = unittest.TestLoader().loadTestsFromTestCase(TestLayers)\n","unittest.TextTestRunner(verbosity=2).run(suite)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}